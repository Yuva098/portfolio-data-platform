{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "416095e1-a519-4221-8569-6d2f997d102d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# input_path = (\n",
    "#     \"abfss://bronze@storageaccountadls01.dfs.core.windows.net/Sales/Customer/Customer.parquet\"\n",
    "# )\n",
    "\n",
    "# df = spark.read.format(\"parquet\").load(input_path)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d32d22c-abb9-4f0a-8091-5909e36d0115",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# from pyspark.sql.functions import from_utc_timestamp, date_format, col\n",
    "# from pyspark.sql.types import TimestampType\n",
    "\n",
    "# df = df.withColumn(\n",
    "#     \"ModifiedDate\",\n",
    "#     date_format(\n",
    "#         from_utc_timestamp(\n",
    "#             col(\"ModifiedDate\").cast(TimestampType()),\n",
    "#             \"UTC\"\n",
    "#         ),\n",
    "#         \"yyyy-MM-dd\"\n",
    "#     )\n",
    "# )\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ce937b-09d3-4b8d-be37-d656dea178f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Import required PySpark functions and types\n",
    "from pyspark.sql.functions import (\n",
    "from_utc_timestamp, date_format, col, trim, when, lower, regexp_replace, isnan, lit)\n",
    "from pyspark.sql.types import TimestampType, IntegerType, FloatType\n",
    "import re\n",
    "# Function to convert column names to snake case for consistency and readability\n",
    "def to_snake_case(name):\n",
    "    name = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n",
    "    name = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name) # Convert Case to snake_case\n",
    "    return name.lower().replace(\" \", \"_\")\n",
    "\n",
    "# Collect all table (folder) names under the Bronze path\n",
    "table_name = []\n",
    "for i in dbutils.fs.ls(\"abfss://bronze@storageaccountadls01.dfs.core.windows.net/Sales/\"):\n",
    "\n",
    "    table_name.append(i.name.replace(\"/\", \"\")) # Clean up folder names.\n",
    "\n",
    "# Loop through each table and apply transformations\n",
    "for i in table_name:\n",
    "# Construct the full path to the Parquet file\n",
    "  input_path = f\"abfss://bronze@storageaccountadls01.dfs.core.windows.net/Sales/{i}/{i}.parquet\"\n",
    "# Read the Parquet file into a Spark DataFrame\n",
    "df = spark.read.format(\"parquet\").load(input_path)\n",
    "# Rename all columns to snake_case\n",
    "for old_col in df.columns:\n",
    "    df = df.withColumnRenamed(old_col, to_snake_case(old_col))\n",
    "# Apply standard transformations\n",
    "for col_name in df.columns:\n",
    "    # Convert date/datetime columns to yyyy-MM-dd format\n",
    "    if \"date\" in col_name:\n",
    "        df = df.withColumn(col_name, date_format(from_utc_timestamp(col(col_name).cast(TimestampType()), \"UTC\"), \"yyyy-MM-dd\"))\n",
    "    \n",
    "    # Trim whitespaces in string columns\n",
    "    if str(df.schema[col_name].dataType) == \"StringType\":\n",
    "        df = df.withColumn(col_name, trim(col(col_name)))\n",
    "    \n",
    "    # Replace null/NaN in string columns with \"Unknown\"\n",
    "    if str(df.schema[col_name].dataType) == \"StringType\":\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull() | isnan(col(col_name)), lit(\"Unknown\")).otherwise(col(col_name))\n",
    "        )\n",
    "# Replace null in integer columns with 0\n",
    "if str(df.schema[col_name].dataType) == \"IntegerType\":\n",
    "    df = df.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name).isNull(), lit(0)).otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "# Replace null in float columns with 0.0\n",
    "if str(df.schema[col_name].dataType) == \"FloatType\":\n",
    "    df = df.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name).isNull(), lit(0.0)).otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "# Remove duplicate rows if any\n",
    "df = df.dropDuplicates()\n",
    "# df.display()\n",
    "# Define output path in Silver container\n",
    "output_path = f\"abfss://silver@forsynapseaccount .dfs.core.windows.net/Sales/{i}/\"\n",
    "# Write the transformed DataFrame to Silver layer in Delta format\n",
    "# df.write.format(\"delta\").mode(\"overwrite\").save(output_path)python\n",
    "for i in table_name:\n",
    "    try:\n",
    "        print(f\"Attempting to process table: {i}\")\n",
    "        # input_path = f\"abfss://bronze@storageaccountadls01.dfs.core.windows.net/Sales/{i}/{i}.parquet\"\n",
    "        # df = spark.read.format(\"parquet\").load(input_path)\n",
    "        # ... (all your transformation code goes here) ...\n",
    "        output_path = f\"abfss://silver@forsynapseaccount.dfs.core.windows.net/Sales/{i}/\"\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "        print(f\"Successfully processed table: {i}\")\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED to process table: {i}\")\n",
    "        # This will print the exact technical error message to your notebook output\n",
    "        print(e) "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronzetosilver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
